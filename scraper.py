import requests
import json
import re
from bs4 import BeautifulSoup
from typing import Dict, Optional

def parse_itmo_program(url: str) -> Dict:
    """
    –ü–∞—Ä—Å–∏—Ç –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –ò–¢–ú–û –ø–æ URL.
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ JSON (__NEXT_DATA__).
    
    Args:
        url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä:
                   https://abit.itmo.ru/program/master/ai
    
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ø—Ä–æ–≥—Ä–∞–º–º–µ.
    
    Raises:
        ValueError: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.
        requests.RequestException: –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–µ—Ç–∏.
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ru,en;q=0.9",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
        html_content = response.text
    except requests.RequestException as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")

    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ <script id="__NEXT_DATA__">
    soup = BeautifulSoup(html_content, 'html.parser')
    script_tag = soup.find('script', id='__NEXT_DATA__')

    if script_tag and script_tag.string:
        json_str = script_tag.string
    else:
        # –ï—Å–ª–∏ —Ç–µ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –∏—â–µ–º –≤—Ä—É—á–Ω—É—é –ø–æ —à–∞–±–ª–æ–Ω—É JSON
        pattern = r'<script[^>]*type\s*=\s*["\']application/json["\'][^>]*>(.*?)</script>'
        matches = re.findall(pattern, html_content, re.DOTALL)
        
        # –ò—â–µ–º —Ç–æ—Ç, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç "props" –∏ "__N_SSG"
        for match in matches:
            if '"props"' in match and '__N_SSG' in match:
                json_str = match
                break
        else:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ __NEXT_DATA__ –∏–ª–∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π JSON —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")

    # –£–±–∏—Ä–∞–µ–º HTML-—Å—É—â–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    json_str = json_str.strip()
    if json_str.startswith('<!--'):
        json_str = re.sub(r'^<!--(.*?)-->$', r'\1', json_str, flags=re.DOTALL).strip()

    # –†–∞—Å–ø–∞—Ä—Å–∏–º JSON
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError as e:
        # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –≤–∞–ª–∏–¥–Ω—ã–π JSON –∏–∑ —Å—Ç—Ä–æ–∫–∏
        try:
            # –ò–Ω–æ–≥–¥–∞ JSON –æ–±—Ä–µ–∑–∞–Ω ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –Ω–∞—á–∞–ª–æ
            start = json_str.find('{"props":')
            end = json_str.rfind('}') + 1
            if start != -1 and end > start:
                clean_json = json_str[start:end]
                data = json.loads(clean_json)
            else:
                raise e
        except:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {e}")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    try:
        page_props = data.get("props", {}).get("pageProps", {})
        if not page_props:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω pageProps –≤ –¥–∞–Ω–Ω—ã—Ö")
        else:
            print("–ï—Å—Ç—å pageProps –≤ –¥–∞–Ω–Ω—ã—Ö")
            print(page_props.keys())
            print(page_props['similarPrograms'])
        program_data = data['props']['pageProps']['apiProgram']
        program_data_2 = data['props']['pageProps']['jsonProgram']
        exam_dates = data['props']['pageProps'].get('examDates', [])
    except KeyError:
        raise ValueError("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π. –í–æ–∑–º–æ–∂–Ω–æ, URL –Ω–µ–≤–µ—Ä–Ω—ã–π.")

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    parsed = {
        "title": program_data.get("title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
        "slug": program_data.get("slug", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
        "about": {
            "lead": program_data_2.get("about", {}).get("lead", ""),
            "desc": program_data_2.get("about", {}).get("desc", "")
        },
        "career": {
            "text": program_data_2.get("career", {}).get("lead", "")
        },
        "education_cost": program_data.get("educationCost", {}),
        "study_period": program_data.get("study", {}).get("label", ""),
        "language": program_data.get("language", ""),
        "academic_plan_url": program_data.get("academic_plan", ""),
        "direction_of_education": program_data.get("direction_of_education", ""),
        "direction_code": program_data.get("direction_code", ""),
        "faculty": program_data.get("faculties", [{}])[0].get("title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
        "faculty_link": program_data.get("faculties", [{}])[0].get("link", ""),
        "manager": {
            "name": f"{page_props.get('supervisor', {}).get('firstName', '')} {page_props.get('supervisor', {}).get('middleName', '')} {page_props.get('supervisor', {}).get('lastName', '')}".strip(),
        },
        "team": [
            {
                "name": f"{member.get('firstName', '')} {member.get('middleName', '')} {member.get('lastName', '')} ".strip(),
                "position": member.get("positions", [{}])[0].get("position_name", "") if member.get("positions") else "",
                "degree": member.get("degree", "")
            }
            for member in page_props.get("team", [])
        ],
        "partners": [
            logo.split('/')[-1] for logo in program_data_2.get("careersImages", [])
        ],
        "scholarships": [
            {"name": "–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è —Å—Ç–∏–ø–µ–Ω–¥–∏—è", "amount": "–î–æ 4 100 —Ä—É–±–ª–µ–π"},
            {"name": "–ü–æ–≤—ã—à–µ–Ω–Ω–∞—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è —Å—Ç–∏–ø–µ–Ω–¥–∏—è", "amount": "–î–æ 27 000 —Ä—É–±–ª–µ–π"},
            {"name": "–°—Ç–∏–ø–µ–Ω–¥–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –∏ –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§", "amount": "–î–æ 30 000 —Ä—É–±–ª–µ–π"},
            {"name": "–ò–º–µ–Ω–Ω–∞—è —Å—Ç–∏–ø–µ–Ω–¥–∏—è –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞", "amount": "7 000 —Ä—É–±–ª–µ–π"},
            {"name": "–°—Ç–∏–ø–µ–Ω–¥–∏—è —Ñ–æ–Ω–¥–∞ –í–ª–∞–¥–∏–º–∏—Ä–∞ –ü–æ—Ç–∞–Ω–∏–Ω–∞", "amount": "25 000 —Ä—É–±–ª–µ–π"},
            {"name": "–°—Ç–∏–ø–µ–Ω–¥–∏—è ¬´–ê–ª—å—Ñ–∞-–®–∞–Ω—Å¬ª", "amount": "–î–æ 300 000 —Ä—É–±–ª–µ–π"}
        ],
        "admission_methods": [
            method.get("title", "") for method in page_props.get("admission", {}).get("items", [])
        ],
        "exam_dates": exam_dates,
        "social_links": program_data_2.get("social", {}),
        "video_links": [video["content"] for video in program_data_2.get("about", {}).get("video", [])],
        "similar_programs": 
           [
            {
                "title": prog.get("title", ""),
                "year": prog.get("year", ""),
                "direction_of_education": prog.get("direction_of_education", ""),
                "slug": prog.get("slug", ""),
            }
            for prog in page_props['similarPrograms']
        ]
    }

    return parsed


# === –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ===
if __name__ == "__main__":
    urls = [
        "https://abit.itmo.ru/program/master/ai",
        "https://abit.itmo.ru/program/master/ai_product"
    ]

    for url in urls:
        try:
            print(f"\nüîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å: {url}")
            data = parse_itmo_program(url)
            filename = f"itmo_{data['slug']}_parsed.json"
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
            print(f"üìå –ù–∞–∑–≤–∞–Ω–∏–µ: {data['title']}")
            print(f"üìÖ –î–∞—Ç—ã —ç–∫–∑–∞–º–µ–Ω–æ–≤: {len(data['exam_dates'])} —à—Ç.")
            print(f"üë• –ö–æ–º–∞–Ω–¥–∞: {len(data['team'])} —á–µ–ª–æ–≤–µ–∫")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
